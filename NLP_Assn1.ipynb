{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA340 HW01\n",
    "Problem Set 01: Understanding Vector Spaces\n",
    "\n",
    "Rija Masroor and Yera Park"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "We have chosen Gensim to be our word2vec model.\n",
    "\n",
    "#### What two algorithms are used to train word2vec models? Explain the differences between the two algorithms?\n",
    "The two algorithms used to train Word2Vec models are called Continuous Bag of Words (CBOW) and Skip-Gram. Both represent words in a continuous vector space. However, CBOW utilizes context words to predict the target word, while Skip-Gram uses the target word to predict the context words.<br />\n",
    "More precisely, CBOW uses a neural network with a single hidden layer where the context words are the input and the predicted target word is the output layer and Skip-Gram has reversed input and output layers. <br />\n",
    "CBOW is more advantageous when the contexts are clear and the order of words is not important whereas Skip-Gram is better with larger datasets and for semantic analysis. <br />\n",
    "The main differences between these algorithms include the reverse directions of target and context words, model complexity, and differing advantageous performance settings.<br />\n",
    "\n",
    "#### Relate the GLoVE model to the word2vec model. What are the differences between the two models?\n",
    "Although they both capture semantic relationships between words, Word2Vec uses neural network architectures with CBOW or Skip-Gram algorithms whereas GloVe uses statistical information as a count-based model. <br />\n",
    "CBOW and Skip-Gram each predict the target and context words with a single hidden layer. GloVe captures co-occurrence statistics of words by matrix analysis. This is done by pairing words according to co-occurrence frequencies and factorizing the matrix. <br />\n",
    "For training, Word2Vec uses a fixed-size window around each target word. However, Glove does not use it but rather uses overall co-occurrence statistics. <br />\n",
    "The accuracy of Word2Vec can be influenced by choice of algorithms and hyperparameter settings whereas GloVe is more advantageous in analyzing global semantic relationships and overall distributional properties of words. <br />\n",
    "Although two models are created for similar goals, Word2Vec comes from neural network architectures and local context prediction and GloVe is a count-based model that utilizes co-occurrence statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\yera\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yera\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\yera\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\yera\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\yera\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: gensim in c:\\users\\yera\\anaconda3\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: sklearn in c:\\users\\yera\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yera\\anaconda3\\lib\\site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\yera\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: scikit-optimize in c:\\users\\yera\\anaconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-optimize) (21.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.7.3)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-optimize) (23.12.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from packaging>=21.3->scikit-optimize) (3.0.4)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\yera\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\yera\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install sklearn\n",
    "!pip install scikit-learn\n",
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yera\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB_Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing (cleaning, tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[one, review, mention, watch, oz, episod, 'll,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production.   The filming t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[wonder, littl, product, film, techniqu, unass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[basic, 's, famili, littl, boy, jake, think, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[petter, mattei, 's, ``, love, time, money, ''...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production.   The filming t...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                    tokenized_review  \n",
       "0  [one, review, mention, watch, oz, episod, 'll,...  \n",
       "1  [wonder, littl, product, film, techniqu, unass...  \n",
       "2  [thought, wonder, way, spend, time, hot, summe...  \n",
       "3  [basic, 's, famili, littl, boy, jake, think, '...  \n",
       "4  [petter, mattei, 's, ``, love, time, money, ''...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove certain words \n",
    "df['review'] = df['review'].apply(lambda x: re.sub('<br />', ' ', x))\n",
    "\n",
    "# Create instances of the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# For stopwords we will add punctuation\n",
    "punct = list(string.punctuation) + list(string.digits) \n",
    "stop_words = stopwords.words('english') + punct + ['null']\n",
    "\n",
    "# Function to tokenize and lemmatize text\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization and lemmatization to the reviews column\n",
    "df['tokenized_review'] = df['review'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized_reviews column to a list\n",
    "tokenized_review_list = df['tokenized_review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized_reviews column to a list\n",
    "sentiment_list = df['sentiment'].tolist()\n",
    "sentiment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word2Vec Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_review_list, vector_size=100, window=5, min_count=5, sg=0, negative=5, epochs=10)\n",
    "\n",
    "# Explanation of Hyperparameter Choices:\n",
    "# - Vector Size: 100 dimensions to balance between capturing semantic information and computational efficiency.\n",
    "# - Window Size: 5 to capture both local and global context within movie reviews.\n",
    "# - Minimum Word Count: 5 to filter out rare words and improve model generalization.\n",
    "# - Training Algorithm: CBOW (sg=0) chosen for simplicity and efficiency.\n",
    "# - Negative Sampling: 5 negative samples to improve training efficiency.\n",
    "# - Epochs: 10 epochs chosen to balance between training duration and model convergence.\n",
    "\n",
    "# Save or use the trained Word2Vec model for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x25a46c58970>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize reviews using the trained Word2Vec model\n",
    "def vectorize_review(review_tokens, model):\n",
    "    vectors = [model.wv[word] for word in review_tokens if word in model.wv.index_to_key]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # return zero vector if review has no known words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([vectorize_review(tokens, word2vec_model) for tokens in tokenized_review_list])\n",
    "y = np.array(sentiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=317)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression: A relatively high accuracy score of 87% is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yera\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression: Not appropriate for sentiment analysis because sentiment analysis is a binary classification task, while linear regression prvides a range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Encode categorical target labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# # Train a linear regression model\n",
    "# linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred_encoded = linear_model.predict(X_test)\n",
    "\n",
    "# # Convert predictions back to original categorical labels\n",
    "# y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# # Evaluate the model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree: Accuracy is lower than for logistic regression, perhaps because it may have overfit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a decision tree classifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision Tree Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modify Word2Vec Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "\n",
    "Vector Size: Increasing the vector size to 150 dimensions to capture more semantic information only increased the accuracy score for logistic regression from 87.02% to 87.73%, a very small different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "word2vec_model_2 = Word2Vec(sentences=tokenized_review_list, vector_size=150, window=5, min_count=5, sg=0, negative=5, epochs=10)\n",
    "\n",
    "# Explanation of Hyperparameter Changes:\n",
    "# - Vector Size: Increased to 150 dimensions to capture more semantic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize reviews using word2vec_model_2\n",
    "X2 = np.array([vectorize_review(tokens, word2vec_model_2) for tokens in tokenized_review_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sentiment_labels contains the sentiment labels (positive or negative) from the original dataset\n",
    "y2 = np.array(sentiment_list)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=317)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yera\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train2, y_train2)\n",
    "\n",
    "# Make predictions\n",
    "y_pred2 = classifier.predict(X_test2)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test2, y_pred2)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "\n",
    "Vector size reduced but used Skip-Gram (sg), which increased accuracy level from 0.8773 (CBOW, 50 more vector_size).\n",
    "\n",
    "This shows that Skip-Gram is more effective for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "word2vec_model_3 = Word2Vec(sentences=tokenized_review_list, vector_size=100, window=5, min_count=5, sg=1, negative=5, epochs=10)\n",
    "\n",
    "# Explanation of Hyperparameter Changes:\n",
    "# - Training Algorithm Use skipgram (sg=1) instead of continuous bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize reviews using word2vec_model_2\n",
    "X3 = np.array([vectorize_review(tokens, word2vec_model_3) for tokens in tokenized_review_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sentiment_labels contains the sentiment labels (positive or negative) from the original dataset\n",
    "y3 = np.array(sentiment_list)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.2, random_state=317)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yera\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train3, y_train3)\n",
    "\n",
    "# Make predictions\n",
    "y_pred3 = classifier.predict(X_test3)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test3, y_pred3)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and test sets\n",
    "X_train_val_4, X_test_4, y_train_val_4, y_test_4 = train_test_split(X, y, test_size=0.2, random_state=317)\n",
    "X_train_4, X_val_4, y_train_4, y_val_4 = train_test_split(X_train_val_4, y_train_val_4, test_size=0.25, random_state=317)  # Split remaining data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "def objective_function(params):\n",
    "    # Extract hyperparameters\n",
    "    C = params['C']\n",
    "    penalty = params['penalty']\n",
    "    \n",
    "    # Train model with hyperparameters\n",
    "    model = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n",
    "    model.fit(X_train_4, y_train_4)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    accuracy = model.score(X_val_4, y_val_4)\n",
    "    \n",
    "    return -accuracy  # Minimize negative accuracy\n",
    "\n",
    "# Define search space\n",
    "param_space = {\n",
    "    'C': (0.01, 10.0, 'log-uniform'),\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Initialize Bayesian optimizer\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=LogisticRegression(solver='liblinear'),\n",
    "    search_spaces=param_space,\n",
    "    n_iter=50,\n",
    "    cv=StratifiedKFold(n_splits=5),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "bayes_search.fit(X_train_4, y_train_4)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = bayes_search.best_params_\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "final_model = LogisticRegression(**best_params, solver='liblinear')\n",
    "final_model.fit(X_train_4, y_train_4)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = final_model.score(X_test_4, y_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Document 1:\n",
      "I read all these reviews on here about how this is a such a good movie. Jeez, this movie was predictable and pretty boring. The acting was below average most of the time, especially by Mckenna. I haven't seen a more pathetic attempt at making someone \"badass\" in a movie. Oh man, this movie was a letdown. I also read somewhere this might be a cult classic. I know there are followers of the director, but this movie was just a average piece of film.  The script was lame, for the most part the acting was lame, this movie was lame.  Oh and pray for the guy that used to be in Cheers. He looks really bad.   The best actor in this movie was probably the guy in Office Space, and he was only in this movie for about 8 minutes.  4/10\n",
      "\n",
      "Similar Document 2:\n",
      "no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "\n",
      "Similar Document 3:\n",
      "I can't believe this movie has an average rating of 7.0! It is a fiendishly bad movie, and I saw it when it was fairly new, and I was in the age group that is supposed to like it!\n",
      "\n",
      "Similar Document 4:\n",
      "Overall this movie is dreadful, and should have never been made. One of the problems with this movie is that there is no link to the audience and the characters, for example, if she is about to be attacked, you want to feel, \"Oh My God, No!\", but you don't in this case, you don't care because there is no link that has been made to know the character. In the trailer, it seemed as though the movie would be great, yet there is no suspense what so ever really. There could have been maybe some mystery but there is not. \"All she has is a toolbox.\" was said on the DVD's back, you would think that it was carefully planned this movie, and cleverly made, but it is not, The ending, was just awful, very straight forward, and pointless too. The acting is either average or below average, maybe even lower. In my opinion it was a waste of an hour of my life. The \"Special Effects\" and sets were average too, nothing special what so ever. There is not much gore, or bloody violence, not much blood is shown. This movie was advertised to make it sound quite amazing, yet really, its not even worth looking for, I do not recommend this to anyone, unless they are easily satisfied, by a few fights and a boring story.\n",
      "\n",
      "Similar Document 5:\n",
      "This could be looked at in many different ways. This movie sucks, its good or its just plain weird. The third one probably explains this movie best. It has strange themes and just has a strange plot. So who else but Christopher Walken would play in this no matter how bad, average or even how good it might be.  The acting was what you would expect especially out of Ben Stiller. Jack Black I have always liked so you know what you will get out of him but this is not bad. Christopher Walken is always off the wall. He is always enjoyable to watch no matter how bad the movie is. Comedy wise it is somewhat funny. This of course meaning that it does have its moments (though very few) but can get a little over top here and there which makes me feel like the movie is just desperate for laughs but of course not in a good way.  The directing was average as well. Barry Levinson is a slightly overrated director and really did not do a good job here. This movie seemed that it had a lot more potential and he did not do much to reach it. Just very average and did not seem like a lot of effort was put into making this film.  The writing is the key to a good comedy. Obviously that means the writing here failed. At best it is below average. Considering it does have its moments it was not too horrible. That is never a good thing to say about a movie though.   If not for Christopher Walken and it stupid ridiculous ending I would have given it a lower rating. He is always quite a character in his movies. Stil this is just a whacked out strange movie with strange characters that really don't go anywhere. Not completely horrible but I would not really recommend it though because it is a very forgettable movie.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['review'])\n",
    "\n",
    "# Calculate cosine similarity between TF-IDF vectors\n",
    "def find_similar_documents(tfidf_vector, tfidf_matrix, top_n=5):\n",
    "    cosine_similarities = cosine_similarity(tfidf_vector, tfidf_matrix).flatten()\n",
    "    # Get indices of top n similar documents\n",
    "    similar_doc_indices = cosine_similarities.argsort()[-top_n:][::-1]\n",
    "    similar_documents = [(idx, cosine_similarities[idx]) for idx in similar_doc_indices]\n",
    "    return similar_documents\n",
    "\n",
    "# Example usage\n",
    "query = \"This movie was average. \"\n",
    "query_vector = tfidf_vectorizer.transform([query])\n",
    "similar_documents = find_similar_documents(query_vector, tfidf_matrix)\n",
    "\n",
    "# Print the most similar documents\n",
    "for i, (idx, similarity) in enumerate(similar_documents, 1):\n",
    "    print(f\"Similar Document {i}:\")\n",
    "    print(df['review'][idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning behind my choice of similarity measure\n",
    "\n",
    "Cosine similarity is generally used in identifying similarities by measuring the cosine of the angle (inner product space) between two vectors. It is an excellent tool in measuring similarity in text analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "#### Explain how the Doc2Vec model works and how it differs from the Word2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a model that represents each document as a vector. It is used to create a vectorised representation of a group of words taken collectively as a single unit.\n",
    "\n",
    "In Doc2Vec, two main architectures are used: Distributed Memory (DM) and Distributed Bag of Words (DBOW). In the DM architecture, the model learns to predict the target word given the context words and a unique document ID. In the DBOW architecture, the model predicts words within a document but ignores the context words. This is analogous to the CBOW and the skip-gram architecture in Word2Vec.\n",
    "\n",
    "In contrast, a Word2vec model represents each word as a vector. While Word2Vec focuses on word-level semantics, Doc2Vec extends this concept to capture both word and document semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'vector_size': 100, 'window': 5, 'min_count': 5, 'epochs': 10} | Accuracy: 0.8064\n",
      "Hyperparameters: {'vector_size': 150, 'window': 5, 'min_count': 5, 'epochs': 50} | Accuracy: 0.8503\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import multiprocessing\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"IMDB_Dataset.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "tagged_corpus = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(df['review'])]\n",
    "\n",
    "# Define hyperparameters to test\n",
    "hyperparameters = [\n",
    "    {'vector_size': 100, 'window': 5, 'min_count': 5, 'epochs': 10},\n",
    "    {'vector_size': 150, 'window': 5, 'min_count': 5, 'epochs': 50}\n",
    "]\n",
    "\n",
    "# Define a function to train and evaluate the model with given hyperparameters\n",
    "def train_and_evaluate_model(params, tagged_corpus):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    model = Doc2Vec(vector_size=params['vector_size'], window=params['window'],\n",
    "                    min_count=params['min_count'], workers=cores-1, epochs=params['epochs'])\n",
    "    model.build_vocab(tagged_corpus)\n",
    "    model.train(tagged_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(model.dv, df['sentiment'], test_size=0.2, random_state=317)\n",
    "\n",
    "    # Train a logistic regression classifier\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Train and evaluate models with different hyperparameters\n",
    "results = []\n",
    "for params in hyperparameters:\n",
    "    accuracy = train_and_evaluate_model(params, tagged_corpus)\n",
    "    results.append({'params': params, 'accuracy': accuracy})\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Hyperparameters: {result['params']} | Accuracy: {result['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 Part 2\n",
    "\n",
    "Document and explain your model's hyperparameters and the reasoning behind your choices.\n",
    "\n",
    "Explanation of Hyperparameter Choices:\n",
    "- Vector Size: 100 and 150 dimensions to balance between capturing semantic information, dimensionality, and computational efficiency. These are a popular choices for Doc2Vec models. Higher values for vector size can capture more detailed relationships but requires more computational power. \n",
    "- Window Size: 5 to capture both local and global context within movie reviews.\n",
    "- Minimum Word Count: 5 to filter out rare words and improve model generalization. \n",
    "- Epochs: 10 and 50 epochs chosen to balance between training duration and model convergence. Iterations for Doc2Vec is more time-consuming than Word2Vec due to the size of datasets. \n",
    "\n",
    "We have tried running hyperparameters with larger parameters (vector_size=300, window=5, min_count=5, workers=-1, epochs=5000), but this took over 2 hours.\n",
    "Most likely because of 5000 for epoch. Also, when there were more than 3 hyperparameters (models) trained, it took way longer than what takes for 2 models.\n",
    "Interestingly, 2 models on average on one laptop took around 9 minutes but on the other laptop, it took over 15 minutes. \n",
    "Doc2Vec seems to use computational power significantly more than Word2Vec so there were limited and long attempts in figuring out the models and right parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Document 1:\n",
      "One of the best TV shows out there, if not the best one. Why? Simple: it has guts to show us real life in prison, without any clich√©s and predictable twists. This is not Prison Break or any other show, actually comparing to Oz the show Sopranos look like story for children's. Profanity, cursing, shots of explicit violence and using drugs, disgusting scenes of male sexual organs and rapes... all this and more in Oz. But this is not the best part of Oz; the characters are the strongest point of this show; they're all excellent and not annoying, despite the fact we are looking at brutal criminals. The actors are excellent, my favorite are the actors who are playing Ryan O'Reilly and Tobias Beecher, because they're so unique and changing their behavior completely. And most of all... the don't have no remorse for their actions. Overall... Oz is amazing show, the best one out there. Forget about CSI and shows about stupid doctors... this is the deal... OZ!\n",
      "\n",
      "Similar Document 2:\n",
      "HBO created this show for purposes of making us see the most realistic view of prison possible and they did a hell of a job. Oz was created by the creators of Homicide who wanted to show a raw version of prison. This show is what launched the idea of every other HBO Original Program such as The Sopranos, Sex and the City, The Wire, Arliss, Deadwood, and Six Feet Under amongst others. Oz is the nickname for the Oswald State Penetentiary, a fictional prison in some US state which is never stated (Though with the accents, crime scenes, and racial distribution NY is assumed). The main prison unit looked at on the show is Emerald City, a seemingly ideal prison unit with more privlages than others thought out by a liberal unit manager named Tim McManus. Overall this show shows us what it is really like if one wishes to survive in prison.<br /><br />There are about 10 gangs shown on Oz. First we have the Muslims, a group of blacks who wish to destroy the injustices of the criminal justice system and help improve living conditions for blacks everywhere. They are led by Kareem Said a black militant minister who wishes to destroy everything racist about the judicial system. As a group they are not so much anti-white but rather anti-injustice. Our second group of blacks is the Homeboys who are essentially the street blacks who wish to keep all the bad ghetto behaviors up and run the drug trade. Their leadership varies mainly because they are always losing members due to violence. In this group, one character who is acted terrifically is Simon Adebisi. Adebisi is an African inmate who is essentially the most frighteningly evil character alive. This gang as a whole gets side help from the Irish at times and is always in conflict with the Latinos and Sicilians for drug distribution purposes. Being that Oz is mostly black, the Homeboys have the most soldiers of any gang inside.<br /><br />The Latinos and Sicilians, like the Homeboys have varying leadership due to violent deaths that occur throughout the show. The Sicilians pretty much have the most substantial say in how any illegal activity gets conducted in Oz. The Latinos make their presence known so that they can at least be coasting well if they are not in control. Unlike the Homeboys however, these gangs do not have as much internal battle for power and are usually more stable when it comes to drug usage. The Irish who are mentioned above are a smaller gang led by a manipulative and snakelike Ryan O'Reily. O'Reily always manages to stay in good graces with all the drug powers and manages to manipulate things in his way whenever he wants. They are in no illegal control but they are at least on good terms with all those who are.<br /><br />Amongst the whiter inmates, we have the Bikers and Aryans. The Bikers are merely a bunch of tattooed drug users who help the Aryans out most of the time. The Aryans are the most hated and hateful gang to most any viewer of Oz. They are led by Vern Schillinger who is amongst the most racist, sickest, and sadistic characters one will ever see. Both gangs control nothing illegal, they just merely let the darker skinned inmates see that they are a substantial threat to anyone who thinks all white inmates are soft. We also have the Others. The Others is a gang of outsider prisoners who are not necessarily a problem to any other inmate. In this group we see Tobias Beecher, a lawyer who accidentally killed a young girl whose life is forever altered by prison. We also see Augustus Hill, a black man bound to a wheelchair for killing a police officer who narrates the show and introduces the audience to every inmate. The character's crimes are shown as they are introduced and Augustus lets us know how long they will be in prison. Finally amongst gangs, there are the Christians and Gays. The Christians merely stay religious to keep from going mental and the Gays are a bunch of cross-dressers who are often raped by other inmates.<br /><br />This show gets in depth on a lot of issues dealing with the criminal justice system and is more explicit than any movie about prison. Since language is unedited, we here more racial epithets and cuss words than we would on any other TV show. Augustus Hill's commentary provides a good way for us to truly understand each and every issue involved with Oz. This show as good as it is is not at all for the light to medium hearted. It explicitly shows drug use and distribution by any means possible, prison rape, murders, fatal stabbings, and general gore than anything anyone else has seen. In my opinion it is the most influential and greatest show ever created but I can see at the same time why other people would be disturbed by this show. If you are at all interested by shows and movies about prison, Oz is a must see.\n",
      "\n",
      "Similar Document 3:\n",
      "OZ is an old TV series released by HBO. It shows the life in one security prison called Oswald, but the main plot is focused on Emerald City, which is one of the prison levels.<br /><br />Oz have an amazing plot and an outstanding cast! There are many of characters and almost all of them are very interesting. Basically they are divided in groups (gangs), the Sicilians, the Black people, the Aryans, the Gays, the Latinos and many others who doesn't have any specific group, but deal with many others like Ryan O'Reily (The Irish).<br /><br />The plot is very well built, there are a lot of conspiracy inside, a lot of fight for the power. But all of this is not just exposed in form of violence, all is very intelligent and smart, nothing happens without a reason, all is connected and very interesting to guess! I really love this series, and who wanna see something intelligent, very well produced with a very good cast and performances, MUST see OZ!\n",
      "\n",
      "Similar Document 4:\n",
      "I admit that I almost gave up on watching TV shows. Why? Because most of them are about doctors, forensics or some girl/boy, who can predict a murder by sitting on the toilet seat. But I must say I was wrong when I watched the first episode of Oz, one of the most brilliant shows on TV until today. Oz is a show about maximum secure prison and shows episodes in life of every cell - mate inside the prison and their frustrations. What I like about this show is the fact it has the courage to show the real life in prison without any annoying characters, stereotypes and unrealistic dialogs. Even thought the characters in the show are not supposed to be heroes, you became quickly attached to them and their life. Sure, their is a lot of violence in this show and some disturbing images of rape and taking drugs, but that's the point of this show; to present how brutal and frustrating can be life in prison... heck, you can't show life in prison like a \"clean environment\" with \"likeable characters\", right? Oz is perfect in every way, the actors are doing an excellent job and the whole hour of this show is going threw really really fast. The only thing that I didn't like in this show was the last season of the show, I think it became too weird and brutal and the finale was not satisfying. But overall Oz is still my favorite show on TV so far, it surpasses Sopranos and million CSI and Dr. House crap with just a main title. Brilliant show which has the balls to show what other shows are taking away from us; realism.\n",
      "\n",
      "Similar Document 5:\n",
      "Oz is the TV show which is intensive non-stop adrenaline. Its a show which is not aimed at a large audience but a specific one as its themes are very adult orientated. It obviously did not achieve mainstream success as this was an impossibility but had much fame with its audience and many famous actors either guest starred or became part of the show.<br /><br />Oz is a series of fictional stories based on a prison located somewhere in the New York state. There are many different ethnic groups which play a somewhat equal role in being represented. These groups are the Muslims, Homeboys, Aryans, Bikers, Italians, Latinos, Irish, Christians, Gays and Others. Some have some form of affiliation with each other such as the Bikers and Aryans whilst the Aryans are the enemy's of numerous groups such as the Muslims and the Homeboys. <br /><br />In Oswald Penitentary (Its official name) there is a policy that not too many members from any one ethnic group may be allowed. It is the maximum security prison and the regular prison where sometimes prisoners are sent to or from in the show is called \"general population\" or \"gen pop\". The main character of the show is Augustus Hill who also narrates the story before and after the main segments, crossed between a documentary type and a biographical one. Hill is bound to wheelchair usage as a consequence of his unknown past. He is part of the \"others\" group which features individuals which don't fit into the other prison gangs.<br /><br />Of the many groups it is the smaller ones which are able to keep themselves avoided by trouble most of the times as the larger ones are busy in conflict with one another. Are few of the most noteworthy characters are as follows: 1)Ryan O'Reilly is in charge of his fellow Irishmen and somehow always voluntarily gets himself involved in lethal activity for the weakest link. 2)Miguel Alvarez is the watched Latino who has constant struggles with coping with his popularity within his own gang and with others. 3)Simon Adebesi is the Nigerian who was the former leader of the Homeboys gang. Hes also very physically quite large and solid. These two reasons are why he is one of the most powerful inmates inside of Oz. 4)Karim Said is the Muslim and Black nationalist who is a very defined in his opinions and desires. He fights for the rights of his people on many levels through the show, from his own Muslim group, to his wider Black group and even fighting for all the groups against the unjust rules of authority. He character seems in some ways identical to Malcom X, the famous Black rights fighter who was also Muslim.<br /><br />All in all, for its genre Oz is possibly the greatest (fictional prison show or show based on ethnic relations). If it doesn't sound as if your own type of series it probably shouldn't be tried out, but if it does interest, then it is definitely worth viewing. The biggest problem with the show is how it is addictive, some may feel wrong viewing a lot of this kind of material. Researchers argue on whether it is harmful or not but in the end it is just a reflection of the society of the world we live in today. This means that it is as an informant and an indicator of the world than an aggressor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit-transform the review texts to TF-IDF vectors\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['review'])\n",
    "\n",
    "# Get the TF-IDF vector for a given document (e.g., document at index 0)\n",
    "given_document_index = 0\n",
    "given_tfidf_vector = tfidf_matrix[given_document_index]\n",
    "\n",
    "# Calculate the cosine similarity between the given TF-IDF vector and all other TF-IDF vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(given_tfidf_vector, tfidf_matrix)\n",
    "\n",
    "# Sort the similarities to find the most similar documents\n",
    "most_similar_indices = similarities.argsort()[0][::-1]\n",
    "\n",
    "# Print the most similar documents\n",
    "num_similar_documents = 5\n",
    "for i in range(1, num_similar_documents + 1):\n",
    "    similar_document_index = most_similar_indices[i]\n",
    "    similar_document = df.iloc[similar_document_index]['review']\n",
    "    print(f\"Similar Document {i}:\")\n",
    "    print(similar_document)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Part 2\n",
    "\n",
    "Explain the reasoning behind your choice of similarity measure.\n",
    "\n",
    "Cosine similarity is the most suitable choice for measuring similarity when using Word2Vec models because it is normalized and captures the directional element of vectors. The word frequency and document length may vary but normalization ensures that the orientations of the vectors are emphasized more than the magnitude of vectors. By measuring the inner product space between vectors, cosine similarity is able to capture the semantic similarity between words that have similar meanings and point in similar directions. In addition, cosine similarity is computationally efficient and can thus be used with large datasets. \n",
    "As cosine similarity indicates how close words or documents are and normalizes the length and complexity of words and documents, it is an excellent choice for both Word2Vec and Doc2Vec. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5: Explore the properties of subspaces and their relationship to vector spaces.**\n",
    "\n",
    "**Explain the properties of subspaces and their relationship to vector spaces.**\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subspace is a vector space that is contained within another vector space. So every subspace is a vector space in its own right, but it is also defined relative to some other (larger) vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide an example of a subspace and its relationship to a vector space.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the vector space V = R3 (the real coordinate space over the field R of real numbers), take W to be the set of all vectors in V whose last component is 0. Then W is a subspace of V.\n",
    "\n",
    "Proof:\n",
    "\n",
    "    Given u and v in W, then they can be expressed as u = (u1, u2, 0) and v = (v1, v2, 0). Then u + v = (u1+v1, u2+v2, 0+0) = (u1+v1, u2+v2, 0). Thus, u + v is an element of W, too.\n",
    "    Given u in W and a scalar c in R, if u = (u1, u2, 0) again, then cu = (cu1, cu2, c0) = (cu1, cu2,0). Thus, cu is an element of W too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6 Define a static word vector and a dynamic word vector. Explain the differences between the two types of word vectors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static vector: A fixed representation of a word in a vector space that does not change during the course of a specific task or model training. \n",
    "\n",
    "Dynamic vector: A word representation in vector space that is context-dependent.\n",
    "\n",
    "Differences:\n",
    "Static word vectors consider only sematics, while dynamic vectors consider both sematics and pragamatics. Static vectors are fixed and can be reused across multile tasks without modification while dynamic word vectors are more adaptive acorss contexts. Dynamic wrd vectors are thus better suited for tasks that consider contextual information than static representations,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
